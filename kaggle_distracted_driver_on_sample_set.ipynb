{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu0')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#utils modules contains lots of helper functions from the fast.ai course\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "\n",
    "# Forcing us to use print function and proper division sign\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../kaggle/state-farm-distracted-driver-detection'\n",
    "sample_dir = path + '/sample'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np # linear algebra\n",
    "img_size_1D = 224\n",
    "img_target_size = (img_size_1D, img_size_1D)\n",
    "img_batch_size = 64\n",
    "\n",
    "def load_in_batches(dir_name, gen=image.ImageDataGenerator(), shuffle=False, \n",
    "                    batch_size=4, class_mode='categorical', target_size=img_target_size):\n",
    "    return gen.flow_from_directory(dir_name, target_size=target_size, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1803 images belonging to 10 classes.\n",
      "Found 448 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "sample_train_batches = load_in_batches(sample_dir + '/train', shuffle=True, batch_size=img_batch_size)\n",
    "sample_val_batches = load_in_batches(sample_dir + '/validation', shuffle=False, batch_size=img_batch_size * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's import everything we will need from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, BatchNormalization, Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = sample_train_batches.nb_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_model():\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(axis=1, input_shape=(3, img_size_1D, img_size_1D)))\n",
    "   \n",
    "    model.add(Flatten())       \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_2 (BatchNormal(None, 3, 224, 224)   6           batchnormalization_input_2[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 150528)        0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10)            1505290     flatten_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1505296\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = linear_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1803/1803 [==============================] - 33s - loss: 4.9884 - acc: 0.3272 - val_loss: 8.9146 - val_acc: 0.1875\n",
      "Epoch 2/3\n",
      "1803/1803 [==============================] - 22s - loss: 2.9969 - acc: 0.6578 - val_loss: 6.7202 - val_acc: 0.2612\n",
      "Epoch 3/3\n",
      "1803/1803 [==============================] - 23s - loss: 2.5178 - acc: 0.7759 - val_loss: 4.9194 - val_acc: 0.3661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07b24c3950>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with a low enough learning rate\n",
    "model.optimizer.lr = 10e-5\n",
    "# Now train it for a few epochs\n",
    "model.fit_generator(sample_train_batches, samples_per_epoch=sample_train_batches.nb_sample, nb_epoch=3, \n",
    "                   validation_data=sample_val_batches, nb_val_samples=sample_val_batches.nb_sample, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1803/1803 [==============================] - 29s - loss: 2.4346 - acc: 0.8209 - val_loss: 4.9203 - val_acc: 0.3482\n",
      "Epoch 2/4\n",
      "1803/1803 [==============================] - 23s - loss: 2.1354 - acc: 0.8630 - val_loss: 5.1341 - val_acc: 0.3080\n",
      "Epoch 3/4\n",
      "1803/1803 [==============================] - 24s - loss: 2.3261 - acc: 0.8575 - val_loss: 4.7532 - val_acc: 0.3705\n",
      "Epoch 4/4\n",
      "1803/1803 [==============================] - 24s - loss: 2.1903 - acc: 0.8641 - val_loss: 4.5360 - val_acc: 0.3705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07b24c3b10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increase the learning rate as we seem to be on the right track\n",
    "model.optimizer.lr = 10e-3\n",
    "# Now train it for a few more epochs\n",
    "model.fit_generator(sample_train_batches, samples_per_epoch=sample_train_batches.nb_sample, nb_epoch=4, \n",
    "                   validation_data=sample_val_batches, nb_val_samples=sample_val_batches.nb_sample, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, our model's accuracy on the validation set seems to be stagnating - and a bit unstable. The training set accuracy is going down quite well on its side, showing signs of overfit. \n",
    "We should look at a more powerful model to see if we can decrease the overfit and increase its generic application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a simple ConvNet\n",
    "The convnet contains only two convolutional layers, and is therefore not very deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_convnet():\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,img_size_1D,img_size_1D)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Flatten(),\n",
    "        # We could try changing this - not sure it would massively change the results though    \n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_3 (BatchNormal(None, 3, 224, 224)   6           batchnormalization_input_3[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 32, 222, 222)  896         batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNormal(None, 32, 222, 222)  64          convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 32, 74, 74)    0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 72, 72)    18496       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNormal(None, 64, 72, 72)    128         convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 64, 24, 24)    0           batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 36864)         0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 200)           7373000     flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNormal(None, 200)           400         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            2010        batchnormalization_6[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 7395000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simple_convnet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1803/1803 [==============================] - 33s - loss: 2.1803 - acc: 0.3095 - val_loss: 2.4526 - val_acc: 0.1027\n",
      "Epoch 2/3\n",
      "1803/1803 [==============================] - 28s - loss: 1.1760 - acc: 0.6522 - val_loss: 2.2902 - val_acc: 0.1317\n",
      "Epoch 3/3\n",
      "1803/1803 [==============================] - 25s - loss: 0.6809 - acc: 0.8619 - val_loss: 2.3412 - val_acc: 0.1406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0795d16ed0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = simple_convnet()\n",
    "\n",
    "model.fit_generator(sample_train_batches, samples_per_epoch=sample_train_batches.nb_sample, nb_epoch=3, \n",
    "                   validation_data=sample_val_batches, nb_val_samples=sample_val_batches.nb_sample, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1803/1803 [==============================] - 32s - loss: 0.4437 - acc: 0.9357 - val_loss: 2.3879 - val_acc: 0.1719\n",
      "Epoch 2/4\n",
      "1803/1803 [==============================] - 26s - loss: 0.3073 - acc: 0.9656 - val_loss: 2.3894 - val_acc: 0.2009\n",
      "Epoch 3/4\n",
      "1803/1803 [==============================] - 28s - loss: 0.2305 - acc: 0.9778 - val_loss: 2.3776 - val_acc: 0.2031\n",
      "Epoch 4/4\n",
      "1803/1803 [==============================] - 27s - loss: 0.1670 - acc: 0.9922 - val_loss: 2.3154 - val_acc: 0.2143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0795d18690>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 10e-3\n",
    "\n",
    "model.fit_generator(sample_train_batches, samples_per_epoch=sample_train_batches.nb_sample, nb_epoch=4, \n",
    "                   validation_data=sample_val_batches, nb_val_samples=sample_val_batches.nb_sample, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No luck at all here. Our model overfits even more and converges much faster towards perfect accuracy on training sample. We need to make it work harder to spend more time distilling the import features that will enable it to improve accuracy on the validation sample set.\n",
    "We will use data augmentation for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "We need to try different types of data augmentation, one at a time, to determine what parameters work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# But first, let's combine the steps above into a function\n",
    "def create_and_run_simple_convnet(t_batch=sample_train_batches, v_batch=sample_val_batches):\n",
    "    model = simple_convnet()\n",
    "\n",
    "    model.fit_generator(t_batch, samples_per_epoch=t_batch.nb_sample, nb_epoch=5, \n",
    "                       validation_data=v_batch, nb_val_samples=v_batch.nb_sample, verbose=1)\n",
    "\n",
    "    model.optimizer.lr = 10e-3\n",
    "\n",
    "    model.fit_generator(t_batch, samples_per_epoch=t_batch.nb_sample, nb_epoch=2, \n",
    "                       validation_data=v_batch, nb_val_samples=v_batch.nb_sample, verbose=1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Width_Shift_Augmentations\n",
    "Here is image is moved left and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1803 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_ws = image.ImageDataGenerator(width_shift_range=0.15)\n",
    "sample_train_batches = load_in_batches(sample_dir + '/train', shuffle=True, gen=gen_ws, batch_size=img_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1803/1803 [==============================] - 32s - loss: 2.2328 - acc: 0.2806 - val_loss: 2.3781 - val_acc: 0.1406\n",
      "Epoch 2/5\n",
      "1803/1803 [==============================] - 25s - loss: 1.1289 - acc: 0.6800 - val_loss: 2.4277 - val_acc: 0.1607\n",
      "Epoch 3/5\n",
      "1803/1803 [==============================] - 23s - loss: 0.6476 - acc: 0.8874 - val_loss: 2.5304 - val_acc: 0.1473\n",
      "Epoch 4/5\n",
      "1803/1803 [==============================] - 27s - loss: 0.4145 - acc: 0.9440 - val_loss: 2.5956 - val_acc: 0.1518\n",
      "Epoch 5/5\n",
      "1803/1803 [==============================] - 25s - loss: 0.2855 - acc: 0.9756 - val_loss: 2.6248 - val_acc: 0.1585\n",
      "Epoch 1/2\n",
      "1803/1803 [==============================] - 30s - loss: 0.1906 - acc: 0.9900 - val_loss: 2.6090 - val_acc: 0.1607\n",
      "Epoch 2/2\n",
      "1803/1803 [==============================] - 27s - loss: 0.1469 - acc: 0.9945 - val_loss: 2.5739 - val_acc: 0.1763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f079421c110>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_and_run_simple_convnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Height Shift Augmentations\n",
    "Here is image is moved up and down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1803 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "1803/1803 [==============================] - 33s - loss: 2.2395 - acc: 0.2845 - val_loss: 2.4512 - val_acc: 0.1272\n",
      "Epoch 2/5\n",
      "1803/1803 [==============================] - 27s - loss: 1.1973 - acc: 0.6556 - val_loss: 2.2927 - val_acc: 0.1496\n",
      "Epoch 3/5\n",
      "1803/1803 [==============================] - 26s - loss: 0.7129 - acc: 0.8647 - val_loss: 2.3315 - val_acc: 0.1161\n",
      "Epoch 4/5\n",
      "1803/1803 [==============================] - 26s - loss: 0.4591 - acc: 0.9379 - val_loss: 2.3608 - val_acc: 0.1116\n",
      "Epoch 5/5\n",
      "1803/1803 [==============================] - 26s - loss: 0.3085 - acc: 0.9717 - val_loss: 2.3672 - val_acc: 0.1138\n",
      "Epoch 1/2\n",
      "1803/1803 [==============================] - 31s - loss: 0.2163 - acc: 0.9878 - val_loss: 2.3345 - val_acc: 0.1295\n",
      "Epoch 2/2\n",
      "1803/1803 [==============================] - 27s - loss: 0.1628 - acc: 0.9956 - val_loss: 2.2856 - val_acc: 0.1696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f079169e350>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_hs = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "sample_train_batches = load_in_batches(sample_dir + '/train', shuffle=True, gen=gen_hs, batch_size=img_batch_size)\n",
    "create_and_run_simple_convnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shear Angles Augmentations\n",
    "In a shear transformation, points are proportionally displaced to map onto a new plane whose lines are that of the original plane + some rotation angle. Keras uses radians for the range. Check out [Wikipedia](https://en.wikipedia.org/wiki/Shear_mapping) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1803 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "1803/1803 [==============================] - 32s - loss: 2.2506 - acc: 0.2768 - val_loss: 2.3205 - val_acc: 0.1138\n",
      "Epoch 2/5\n",
      "1803/1803 [==============================] - 24s - loss: 1.1949 - acc: 0.6534 - val_loss: 2.2926 - val_acc: 0.1652\n",
      "Epoch 3/5\n",
      "1803/1803 [==============================] - 26s - loss: 0.6900 - acc: 0.8630 - val_loss: 2.3246 - val_acc: 0.1674\n",
      "Epoch 4/5\n",
      "1803/1803 [==============================] - 25s - loss: 0.4346 - acc: 0.9401 - val_loss: 2.3643 - val_acc: 0.1317\n",
      "Epoch 5/5\n",
      "1803/1803 [==============================] - 26s - loss: 0.3091 - acc: 0.9651 - val_loss: 2.4020 - val_acc: 0.1138\n",
      "Epoch 1/2\n",
      "1803/1803 [==============================] - 31s - loss: 0.2057 - acc: 0.9900 - val_loss: 2.3838 - val_acc: 0.1317\n",
      "Epoch 2/2\n",
      "1803/1803 [==============================] - 25s - loss: 0.1608 - acc: 0.9967 - val_loss: 2.3734 - val_acc: 0.1473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f078f122a90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_sh = image.ImageDataGenerator(shear_range=0.1)\n",
    "sample_train_batches = load_in_batches(sample_dir + '/train', shuffle=True, gen=gen_sh, batch_size=img_batch_size)\n",
    "create_and_run_simple_convnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotation Angle Augmentation\n",
    "The image is simply randomly rotated by a degrees values in the range submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1803 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "1803/1803 [==============================] - 31s - loss: 2.2863 - acc: 0.2795 - val_loss: 2.3322 - val_acc: 0.1384\n",
      "Epoch 2/5\n",
      "1803/1803 [==============================] - 24s - loss: 1.1066 - acc: 0.6883 - val_loss: 2.2767 - val_acc: 0.1429\n",
      "Epoch 3/5\n",
      "1803/1803 [==============================] - 26s - loss: 0.6399 - acc: 0.8741 - val_loss: 2.2968 - val_acc: 0.1384\n",
      "Epoch 4/5\n",
      "1803/1803 [==============================] - 25s - loss: 0.4168 - acc: 0.9401 - val_loss: 2.3031 - val_acc: 0.1518\n",
      "Epoch 5/5\n",
      "1803/1803 [==============================] - 26s - loss: 0.2768 - acc: 0.9834 - val_loss: 2.2669 - val_acc: 0.1875\n",
      "Epoch 1/2\n",
      "1803/1803 [==============================] - 32s - loss: 0.1918 - acc: 0.9917 - val_loss: 2.2409 - val_acc: 0.2098\n",
      "Epoch 2/2\n",
      "1803/1803 [==============================] - 25s - loss: 0.1495 - acc: 0.9956 - val_loss: 2.2105 - val_acc: 0.2232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f078993b990>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_rt = image.ImageDataGenerator(rotation_range=15)\n",
    "sample_train_batches = load_in_batches(sample_dir + '/train', shuffle=True, gen=gen_rt, batch_size=img_batch_size)\n",
    "create_and_run_simple_convnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channel Shift Augmentation\n",
    "Randomly shifting the R,G,B color channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1803 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "1803/1803 [==============================] - 33s - loss: 2.1001 - acc: 0.3178 - val_loss: 2.3843 - val_acc: 0.1116\n",
      "Epoch 2/5\n",
      "1803/1803 [==============================] - 24s - loss: 1.1510 - acc: 0.6705 - val_loss: 2.2682 - val_acc: 0.1674\n",
      "Epoch 3/5\n",
      "1803/1803 [==============================] - 23s - loss: 0.6533 - acc: 0.8686 - val_loss: 2.3285 - val_acc: 0.1473\n",
      "Epoch 4/5\n",
      "1803/1803 [==============================] - 24s - loss: 0.4210 - acc: 0.9445 - val_loss: 2.4200 - val_acc: 0.1362\n",
      "Epoch 5/5\n",
      "1803/1803 [==============================] - 25s - loss: 0.3038 - acc: 0.9673 - val_loss: 2.4801 - val_acc: 0.1518\n",
      "Epoch 1/2\n",
      "1803/1803 [==============================] - 32s - loss: 0.2168 - acc: 0.9872 - val_loss: 2.4694 - val_acc: 0.1562\n",
      "Epoch 2/2\n",
      "1803/1803 [==============================] - 24s - loss: 0.1654 - acc: 0.9945 - val_loss: 2.4729 - val_acc: 0.1830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f0776653b50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_ch = image.ImageDataGenerator(channel_shift_range=10)\n",
    "sample_train_batches = load_in_batches(sample_dir + '/train', shuffle=True, gen=gen_ch, batch_size=img_batch_size)\n",
    "create_and_run_simple_convnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining all augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1803 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "1803/1803 [==============================] - 32s - loss: 2.0674 - acc: 0.3200 - val_loss: 2.3414 - val_acc: 0.1652\n",
      "Epoch 2/5\n",
      "1803/1803 [==============================] - 24s - loss: 1.0091 - acc: 0.7232 - val_loss: 2.2138 - val_acc: 0.1942\n",
      "Epoch 3/5\n",
      "1803/1803 [==============================] - 26s - loss: 0.5925 - acc: 0.8913 - val_loss: 2.2152 - val_acc: 0.1763\n",
      "Epoch 4/5\n",
      "1803/1803 [==============================] - 28s - loss: 0.3844 - acc: 0.9468 - val_loss: 2.1976 - val_acc: 0.1942\n",
      "Epoch 5/5\n",
      "1803/1803 [==============================] - 29s - loss: 0.2558 - acc: 0.9756 - val_loss: 2.1718 - val_acc: 0.2232\n",
      "Epoch 1/2\n",
      "1803/1803 [==============================] - 32s - loss: 0.1768 - acc: 0.9906 - val_loss: 2.1450 - val_acc: 0.2612\n",
      "Epoch 2/2\n",
      "1803/1803 [==============================] - 27s - loss: 0.1384 - acc: 0.9983 - val_loss: 2.1134 - val_acc: 0.2812\n"
     ]
    }
   ],
   "source": [
    "gen_all = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, shear_range=0.15, channel_shift_range=10, width_shift_range=0.1)\n",
    "sample_train_batches = load_in_batches(sample_dir + '/train', shuffle=True, gen=gen_all, batch_size=img_batch_size)\n",
    "model = create_and_run_simple_convnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not looking that good. Let's change the learning rate and see if we improve the accuracy on validation sample set. In my original notebook the accuracy on training sample set was much lower. Maybe it's because the augmentations are applied at random and I was luckier in my previous spell as \"better\" augmentation were applied..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1803/1803 [==============================] - 32s - loss: 2.1090 - acc: 0.3361 - val_loss: 2.0389 - val_acc: 0.2946\n",
      "Epoch 2/3\n",
      "1803/1803 [==============================] - 29s - loss: 1.8628 - acc: 0.3988 - val_loss: 1.9312 - val_acc: 0.3549\n",
      "Epoch 3/3\n",
      "1803/1803 [==============================] - 27s - loss: 1.6985 - acc: 0.4365 - val_loss: 1.8381 - val_acc: 0.4018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f077574a190>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(sample_train_batches, samples_per_epoch=sample_train_batches.nb_sample, nb_epoch=3, \n",
    "                   validation_data=sample_val_batches, nb_val_samples=sample_val_batches.nb_sample, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is encouraging... Let's try more epochs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1803/1803 [==============================] - 34s - loss: 1.5848 - acc: 0.4759 - val_loss: 1.7262 - val_acc: 0.4598\n",
      "Epoch 2/15\n",
      "1803/1803 [==============================] - 28s - loss: 1.4848 - acc: 0.5058 - val_loss: 1.7219 - val_acc: 0.4330\n",
      "Epoch 3/15\n",
      "1803/1803 [==============================] - 27s - loss: 1.4427 - acc: 0.5197 - val_loss: 1.6885 - val_acc: 0.4643\n",
      "Epoch 4/15\n",
      "1803/1803 [==============================] - 26s - loss: 1.3390 - acc: 0.5591 - val_loss: 1.6704 - val_acc: 0.4821\n",
      "Epoch 5/15\n",
      "1803/1803 [==============================] - 28s - loss: 1.2676 - acc: 0.5801 - val_loss: 1.6385 - val_acc: 0.5022\n",
      "Epoch 6/15\n",
      "1803/1803 [==============================] - 27s - loss: 1.2019 - acc: 0.6140 - val_loss: 1.6648 - val_acc: 0.5268\n",
      "Epoch 7/15\n",
      "1803/1803 [==============================] - 26s - loss: 1.2263 - acc: 0.6001 - val_loss: 1.6682 - val_acc: 0.5513\n",
      "Epoch 8/15\n",
      "1803/1803 [==============================] - 28s - loss: 1.1575 - acc: 0.6251 - val_loss: 1.6483 - val_acc: 0.5536\n",
      "Epoch 9/15\n",
      "1803/1803 [==============================] - 27s - loss: 1.1017 - acc: 0.6495 - val_loss: 1.6532 - val_acc: 0.5201\n",
      "Epoch 10/15\n",
      "1803/1803 [==============================] - 26s - loss: 1.1016 - acc: 0.6367 - val_loss: 1.6890 - val_acc: 0.5045\n",
      "Epoch 11/15\n",
      "1803/1803 [==============================] - 26s - loss: 1.0650 - acc: 0.6450 - val_loss: 1.6126 - val_acc: 0.5513\n",
      "Epoch 12/15\n",
      "1803/1803 [==============================] - 27s - loss: 1.0449 - acc: 0.6755 - val_loss: 1.6023 - val_acc: 0.5446\n",
      "Epoch 13/15\n",
      "1803/1803 [==============================] - 27s - loss: 1.0075 - acc: 0.6772 - val_loss: 1.5325 - val_acc: 0.5558\n",
      "Epoch 14/15\n",
      "1803/1803 [==============================] - 25s - loss: 0.9709 - acc: 0.7005 - val_loss: 1.5670 - val_acc: 0.5558\n",
      "Epoch 15/15\n",
      "1803/1803 [==============================] - 26s - loss: 0.9722 - acc: 0.6877 - val_loss: 1.6254 - val_acc: 0.5580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f077574acd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(sample_train_batches, samples_per_epoch=sample_train_batches.nb_sample, nb_epoch=15, \n",
    "                   validation_data=sample_val_batches, nb_val_samples=sample_val_batches.nb_sample, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1803/1803 [==============================] - 31s - loss: 0.9475 - acc: 0.7005 - val_loss: 1.5854 - val_acc: 0.5379\n",
      "Epoch 2/5\n",
      "1803/1803 [==============================] - 27s - loss: 0.9099 - acc: 0.6999 - val_loss: 1.5498 - val_acc: 0.5826\n",
      "Epoch 3/5\n",
      "1803/1803 [==============================] - 29s - loss: 0.9000 - acc: 0.7232 - val_loss: 1.5532 - val_acc: 0.5714\n",
      "Epoch 4/5\n",
      "1803/1803 [==============================] - 28s - loss: 0.8898 - acc: 0.7138 - val_loss: 1.5516 - val_acc: 0.5915\n",
      "Epoch 5/5\n",
      "1803/1803 [==============================] - 27s - loss: 0.8577 - acc: 0.7343 - val_loss: 1.6051 - val_acc: 0.5915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f077575e790>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increasing learning rate... Let's see what happens\n",
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(sample_train_batches, samples_per_epoch=sample_train_batches.nb_sample, nb_epoch=5, \n",
    "                   validation_data=sample_val_batches, nb_val_samples=sample_val_batches.nb_sample, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model seems to be reaching its limits - we could probably get slightly better results but we are clearly seeing diminishing returns on the validation set. Let's try another 5 epochs and stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1803/1803 [==============================] - 31s - loss: 0.7018 - acc: 0.7937 - val_loss: 1.5879 - val_acc: 0.5647\n",
      "Epoch 2/5\n",
      "1803/1803 [==============================] - 27s - loss: 0.6374 - acc: 0.8125 - val_loss: 1.6037 - val_acc: 0.5960\n",
      "Epoch 3/5\n",
      "1803/1803 [==============================] - 27s - loss: 0.6662 - acc: 0.8081 - val_loss: 1.5528 - val_acc: 0.6027\n",
      "Epoch 4/5\n",
      "1803/1803 [==============================] - 29s - loss: 0.6656 - acc: 0.7948 - val_loss: 1.5072 - val_acc: 0.5781\n",
      "Epoch 5/5\n",
      "1803/1803 [==============================] - 27s - loss: 0.6499 - acc: 0.8197 - val_loss: 1.4717 - val_acc: 0.5915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f077574abd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(sample_train_batches, samples_per_epoch=sample_train_batches.nb_sample, nb_epoch=5, \n",
    "                   validation_data=sample_val_batches, nb_val_samples=sample_val_batches.nb_sample, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are still very encouraging for such a simple model! Yet it's definitely reached its limits. We must now use the full dataset to try more powerful techniques and make use of a pre-trained VGG network"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
